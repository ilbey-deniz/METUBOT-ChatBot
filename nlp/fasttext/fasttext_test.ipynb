{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from snowballstemmer import TurkishStemmer\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy.linalg import norm\n",
    "# fasttext.util.download_model('tr', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = lambda q_vector, vector : np.dot(q_vector, vector)/(norm(q_vector)*norm(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model(r'cc.tr.300.bin')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('turkish')\n",
    "CUSTOMWORD_LIST = [\"wifi\", \"section\", \"metu\", \"office\"] # custom word list\n",
    "\n",
    "f = open(\"./answers.json\")\n",
    "ANSWERS = json.load(f)\n",
    "\n",
    "f = open(\"./question_categories.json\")\n",
    "QUESTIONS = json.load(f)\n",
    "\n",
    "QUESTION_VECTORS= {}\n",
    "\n",
    "f = open(\"/Users/ilbey/Documents/ceng/ceng49x/metubot/Elasticsearch/qa_pairs.json\")\n",
    "QA = json.load(f)[\"qa-pairs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor for quesiton strings to transform strings to word arrays.\n",
    "# TODO: a better alternative than snowball stemmer should be used. \n",
    "def preProcessor(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # removes all punctuation\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "\n",
    "    ts = TurkishStemmer()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORD_LIST:\n",
    "            if word in CUSTOMWORD_LIST:\n",
    "                result.append(word)\n",
    "            else:\n",
    "                result.append(ts.stemWord(word))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the quesitons dictionary and returns dictionary of dictionaries \n",
    "# where subdictionaries consists of keys as each term and values as \n",
    "# number of occurencies.\n",
    "# This version treats all quesitons in a category as a whole document.  \n",
    "def TF(questions):\n",
    "    result = {}\n",
    "    for category in questions:\n",
    "        result[category] = {}\n",
    "        for question in questions[category]:\n",
    "            term_arr = preProcessor(question)\n",
    "            for term in term_arr:\n",
    "                if term not in result[category]:\n",
    "                    result[category][term] = 1\n",
    "                else:\n",
    "                    result[category][term] += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalizedTF(questions):\n",
    "    result = {}\n",
    "    tf = TF(questions)\n",
    "    for category in tf:\n",
    "        result[category] = {}\n",
    "        no_of_terms = len(tf[category])\n",
    "        for term in tf[category]:\n",
    "            result[category][term] = tf[category][term] / no_of_terms\n",
    "    return result\n",
    "\n",
    "# Calculates inverse document frequency of a term\n",
    "def IDF(term, questions):\n",
    "    number_of_documents = len(questions)\n",
    "    number_of_occurences = 0\n",
    "    tf = TF(questions)\n",
    "    for category in tf:\n",
    "        # number_of_documents += len(tf) # counting all questions, not categories; this can be changed\n",
    "        if term in tf[category]:\n",
    "            number_of_occurences+=1\n",
    "    if number_of_occurences == 0:\n",
    "        return 0\n",
    "    return 1 + np.log(number_of_documents / number_of_occurences)\n",
    "\n",
    "\n",
    "def TFxIDF(terms_string, questions):\n",
    "    result = {}\n",
    "    normal_tf = normalizedTF(questions)\n",
    "    terms = preProcessor(terms_string)\n",
    "\n",
    "    for category in normal_tf:\n",
    "        result[category] = {}\n",
    "        for term in terms:\n",
    "            idf = IDF(term, questions)\n",
    "            if term in normal_tf[category]:\n",
    "                result[category][term] = normal_tf[category][term] * idf\n",
    "            else:\n",
    "                result[category][term] = 0\n",
    "    return result\n",
    "\n",
    "# returns the most likely category as an heuristic for fasttext classifier\n",
    "def categoryHeuristic(query, questions):\n",
    "    tfidf = TFxIDF(query, questions)\n",
    "    max_val = -1\n",
    "    max_category = \"\"\n",
    "    for category in tfidf:\n",
    "        for term in tfidf[category]:\n",
    "            if tfidf[category][term] > max_val:\n",
    "                max_category = category\n",
    "                max_val = tfidf[category][term]\n",
    "    return max_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionVectors(ft, raw_questions):\n",
    "    question_vectors = {}\n",
    "    for key in raw_questions:\n",
    "        question_vectors[key] = []\n",
    "        for q in raw_questions[key]:\n",
    "            question_vectors[key].append(ft.get_sentence_vector(q))\n",
    "    return question_vectors\n",
    "\n",
    "# adds a new field 'q_vectors' \n",
    "# that stores sentence vectors of each question.\n",
    "# IMPORTANT: assign the result to QA constant array  \n",
    "def NEWgetQuestionVectors(ft, questions_answers):\n",
    "    for qa_pair in questions_answers:\n",
    "        if \"q_vectors\" not in qa_pair:\n",
    "            qa_pair[\"q_vectors\"] = []\n",
    "        for q in range(len(qa_pair[\"question\"])):\n",
    "            qa_pair[\"q_vectors\"].append(ft.get_sentence_vector(qa_pair[\"question\"][q]))\n",
    "    return questions_answers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questionClassifier(ft, user_question, question_vectors, raw_questions):\n",
    "    q_vector = ft.get_sentence_vector(user_question)\n",
    "\n",
    "    most_similar_question = \"\"\n",
    "    most_similar_category = \"\"\n",
    "\n",
    "    max_similarity = 0\n",
    "\n",
    "    tfidf_category = categoryHeuristic(user_question, raw_questions)\n",
    "\n",
    "    for key in question_vectors:\n",
    "        for vector, question in zip(question_vectors[key], raw_questions[key]):\n",
    "            sim = cos_sim(q_vector, vector)\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_question = question\n",
    "                most_similar_category = key\n",
    "\n",
    "    if most_similar_category != tfidf_category:\n",
    "        print(\"--MUHTEMEL YANLIS ANLAMALAR SOZ KONUSU--\")\n",
    "        print(\"TF.IDF kategori tahmini: %s\" % tfidf_category)\n",
    "        print(\"FastText kategori tahmini: %s\" % most_similar_category)\n",
    "        # search for the question under the tfidf heuristic category\n",
    "        most_similar_question = \"\"\n",
    "        most_similar_category = \"\"\n",
    "\n",
    "        max_similarity = 0\n",
    "\n",
    "        most_similar_category = tfidf_category\n",
    "        for vector, question in zip(question_vectors[tfidf_category], raw_questions[tfidf_category]):\n",
    "            sim = cos_sim(q_vector, vector)\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_question = question\n",
    "\n",
    "    else:\n",
    "        print(\"Soru kategorisi: %s\" % most_similar_category)\n",
    "        print(\"En yakin soru: %s\" % most_similar_question)\n",
    "\n",
    "    return (most_similar_category, most_similar_question)\n",
    "\n",
    "def NEWquestionClassifier(ft, user_question, questions_answers):\n",
    "    q_vector = ft.get_sentence_vector(user_question)\n",
    "\n",
    "    most_similar_question = \"\"\n",
    "    most_similar_category = \"\"\n",
    "    most_similar_indice = 0\n",
    "\n",
    "    max_similarity = 0\n",
    "\n",
    "    for qa in questions_answers:\n",
    "        for q in range(len(qa[\"q_vectors\"])):\n",
    "            sim = cos_sim(q_vector, qa[\"q_vectors\"][q])\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_question = qa[\"question\"][q]\n",
    "                most_similar_category = qa[\"category\"]\n",
    "                most_similar_indice = questions_answers.index(qa)\n",
    "    \n",
    "    print(\"Benzerlik skoru: \" + str(max_similarity))\n",
    "    print(\"Soru kategorisi: %s\" % most_similar_category)\n",
    "    print(\"En yakin soru: %s\" % most_similar_question)\n",
    "\n",
    "    return most_similar_category, most_similar_question, most_similar_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswer(ft, user_question):\n",
    "    global QUESTION_VECTORS\n",
    "\n",
    "    if QUESTION_VECTORS=={}:\n",
    "        QUESTION_VECTORS = getQuestionVectors(ft, QUESTIONS)\n",
    "    category,question = questionClassifier(ft, user_question, question_vectors=QUESTION_VECTORS, raw_questions=QUESTIONS)\n",
    "    ans = ANSWERS[category]\n",
    "    return ans\n",
    "\n",
    "def NEWgetAnswer(ft, user_question):\n",
    "    global QA\n",
    "    QA = NEWgetQuestionVectors(ft, QA)\n",
    "    category, question, q_index = NEWquestionClassifier(ft, user_question, questions_answers=QA)\n",
    "    ans = random.choice(QA[q_index][\"answer\"])\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benzerlik skoru: 0.5836551\n",
      "Soru kategorisi: kimlik kart\n",
      "En yakin soru: Akıllı kimlik kartımı kaybettim/çalındı, ne yapmalıyım?\n",
      "cevap: Akıllı kimlik kartınızı kaybettiğinizi/çalındığını fark ettiğinizde, cardinfo.metu.edu.tr adresinden kartınızı iptal etmelisiniz. Bu durumda akıllı kimlik kartınız sistem tarafından “iptal” edilir.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"cevap: \" + NEWgetAnswer(ft, \"kartımı bulamıyorum\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
