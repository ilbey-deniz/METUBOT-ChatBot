{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from snowballstemmer import TurkishStemmer\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "# fasttext.util.download_model('tr', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = lambda q_vector, vector : np.dot(q_vector, vector)/(norm(q_vector)*norm(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model(r'cc.tr.300.bin')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('turkish')\n",
    "CUSTOMWORD_LIST = [\"wifi\", \"section\", \"metu\", \"office\"] # custom word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor for quesiton strings to transform strings to word arrays.\n",
    "# TODO: a better alternative than snowball stemmer should be used. \n",
    "def preProcessor(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # removes all punctuation\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "\n",
    "    ts = TurkishStemmer()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in STOPWORD_LIST:\n",
    "            if word in CUSTOMWORD_LIST:\n",
    "                result.append(word)\n",
    "            else:\n",
    "                result.append(ts.stemWord(word))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the quesitons dictionary and returns dictionary of dictionaries \n",
    "# where subdictionaries consists of keys as each term and values as \n",
    "# number of occurencies.\n",
    "# This version treats all quesitons in a category as a whole document.  \n",
    "def TF(questions):\n",
    "    result = {}\n",
    "    for category in questions:\n",
    "        result[category] = {}\n",
    "        for question in questions[category]:\n",
    "            term_arr = preProcessor(question)\n",
    "            for term in term_arr:\n",
    "                if term not in result[category]:\n",
    "                    result[category][term] = 1\n",
    "                else:\n",
    "                    result[category][term] += 1\n",
    "    return result\n",
    "\n",
    "def normalizedTF(questions):\n",
    "    result = {}\n",
    "    tf = TF(questions)\n",
    "    for category in tf:\n",
    "        result[category] = {}\n",
    "        no_of_terms = len(tf[category])\n",
    "        for term in tf[category]:\n",
    "            result[category][term] = tf[category][term] / no_of_terms\n",
    "    return result\n",
    "\n",
    "# Calculates inverse document frequency of a term\n",
    "def IDF(term, questions):\n",
    "    number_of_documents = len(questions)\n",
    "    number_of_occurences = 0\n",
    "    tf = TF(questions)\n",
    "    for category in tf:\n",
    "        # number_of_documents += len(tf) # counting all questions, not categories; this can be changed\n",
    "        if term in tf[category]:\n",
    "            number_of_occurences+=1\n",
    "    if number_of_occurences == 0:\n",
    "        return 0\n",
    "    return 1 + np.log(number_of_documents / number_of_occurences)\n",
    "\n",
    "\n",
    "def TFxIDF(terms_string, questions):\n",
    "    result = {}\n",
    "    normal_tf = normalizedTF(questions)\n",
    "    terms = preProcessor(terms_string)\n",
    "\n",
    "    for category in normal_tf:\n",
    "        result[category] = {}\n",
    "        for term in terms:\n",
    "            idf = IDF(term, questions)\n",
    "            if term in normal_tf[category]:\n",
    "                result[category][term] = normal_tf[category][term] * idf\n",
    "            else:\n",
    "                result[category][term] = 0\n",
    "    return result\n",
    "\n",
    "# returns the most likely category as an heuristic for fasttext classifier\n",
    "def categoryHeuristic(query, questions):\n",
    "    tfidf = TFxIDF(query, questions)\n",
    "    max_val = -1\n",
    "    max_category = \"\"\n",
    "    for category in tfidf:\n",
    "        for term in tfidf[category]:\n",
    "            if tfidf[category][term] > max_val:\n",
    "                max_category = category\n",
    "                max_val = tfidf[category][term]\n",
    "    return max_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sifre': {'wifi': 0, 'şifre': 0, 'unut': 0.1678889830934488}, 'lisanslÄ± yazÄ±lÄ±mlar': {'wifi': 0, 'şifre': 0, 'unut': 0}, 'wifi': {'wifi': 0.5596299436448293, 'şifre': 0, 'unut': 0}}\n",
      "wifi\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./question_categories.json\")\n",
    "questions = json.load(f)\n",
    "\n",
    "#print(TF(questions))\n",
    "#print(IDF(\"wifi\", questions))\n",
    "print(TFxIDF(\"wifi şifremi unuttum\", questions))\n",
    "print(categoryHeuristic(\"wifi şifremi unuttum\", questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionVectors(raw_questions):\n",
    "    question_vectors = {}\n",
    "    for key in questions:\n",
    "        question_vectors[key] = []\n",
    "        for q in questions[key]:\n",
    "            question_vectors[key].append(ft.get_sentence_vector(q))\n",
    "    return question_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questionClassifier(user_question, question_vectors, raw_quesitons):\n",
    "    q_vector = ft.get_sentence_vector(user_question)\n",
    "\n",
    "    most_similar_question = \"\"\n",
    "    most_similar_category = \"\"\n",
    "\n",
    "    max_similarity = 0\n",
    "\n",
    "    tfidf_category = categoryHeuristic(user_question, raw_quesitons)\n",
    "\n",
    "    for key in question_vectors:\n",
    "        for vector, question in zip(question_vectors[key], questions[key]):\n",
    "            sim = cos_sim(q_vector, vector)\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_question = question\n",
    "                most_similar_category = key\n",
    "\n",
    "    if most_similar_category != tfidf_category:\n",
    "        print(\"--MUHTEMEL YANLIŞ ANLAMALAR SÖZ KONUSU--\")\n",
    "        print(\"TF.IDF kategori tahmini: %s\" % tfidf_category)\n",
    "        print(\"FastText kategori tahmini: %s\" % most_similar_category)\n",
    "        # search for the question under the tfidf heuristic category\n",
    "        most_similar_question = \"\"\n",
    "        most_similar_category = \"\"\n",
    "\n",
    "        max_similarity = 0\n",
    "\n",
    "        most_similar_category = tfidf_category\n",
    "        for vector, question in zip(question_vectors[tfidf_category], questions[tfidf_category]):\n",
    "            sim = cos_sim(q_vector, vector)\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                most_similar_question = question\n",
    "        \n",
    "    else:\n",
    "        print(\"Soru kategorisi: %s\" % most_similar_category) \n",
    "        print(\"En yakın soru: %s\" % most_similar_question)\n",
    "    \n",
    "    return (most_similar_category, most_similar_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./answers.json\")\n",
    "answers = json.load(f)\n",
    "\n",
    "def getAnswer(user_question, question_vectors, raw_quesitons):\n",
    "    category,question = questionClassifier(user_question, question_vectors, raw_quesitons)\n",
    "    ans = answers[category]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--MUHTEMEL YANLIŞ ANLAMALAR SÖZ KONUSU--\n",
      "TF.IDF kategori tahmini: wifi\n",
      "FastText kategori tahmini: sifre\n",
      "--MUHTEMEL YANLIŞ ANLAMALAR SÖZ KONUSU--\n",
      "TF.IDF kategori tahmini: wifi\n",
      "FastText kategori tahmini: sifre\n",
      "cevap: Detalied information can be found at https://faq.cc.metu.edu.tr/groups/wireless-network\n"
     ]
    }
   ],
   "source": [
    "question_vectors = getQuestionVectors(questions)\n",
    "questionClassifier(\"wifi şifremi unuttum\", question_vectors, questions)\n",
    "\n",
    "print(\"cevap: \" + getAnswer(\"wifi şifremi unuttum\", question_vectors, questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c50755aaa3c9eceb6d78f65e641ca4df6c8cac8204c8b849fb06dd010dd9d582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
